
@misc{carequalitycommissioncqcMonitoringNHSAcute2017,
	title = {Monitoring {NHS} acute hospitals {\textbar} {Care} {Quality} {Commission}},
	language = {Large print},
	author = {{Care Quality Commission}},
	year = {2017},
	keywords = {or service name},
}

@techreport{carequalitycommissioncqcNHSAcuteHospitals2014,
	title = {{NHS} acute hospitals: {Statistical} {Methodology}},
	institution = {{Care Quality Commission (CQC)}},
	author = {{Care Quality Commission}},
	year = {2014},
}

@article{griggNullSteadystateDistribution2008,
	title = {The null steady-state distribution of the {CUSUM} statistic},
	doi = {10/bgvkdx},
	journal = {Technometrics : a journal of statistics for the physical, chemical, and engineering sciences},
	author = {Grigg, O. and Spiegelhalter, D.},
	year = {2008},
}

@article{griggOverviewRiskadjustedCharts2004,
	title = {An overview of risk-adjusted charts},
	volume = {167},
	issn = {0964-1998 1467-985X},
	doi = {10/c2n3h6},
	abstract = {Summary. The paper provides an overview of risk-adjusted charts, with examples based on two data sets: the first consisting of outcomes following cardiac surgery and patient factors contributing to the Parsonnet score; the second being age–sex-adjusted death-rates per year under a single general practitioner. Charts presented include the cumulative sum (CUSUM), resetting sequential probability ratio test, the sets method and Shewhart chart. Comparisons between the charts are made. Estimation of the process parameter and two-sided charts are also discussed. The CUSUM is found to be the least efficient, under the average run length (ARL) criterion, of the resetting sequential probability ratio test class of charts, but the ARL criterion is thought not to be sensible for comparisons within that class. An empirical comparison of the sets method and CUSUM, for binary data, shows that the sets method is more efficient when the in-control ARL is small and more efficient for a slightly larger range of in-control ARLs when the change in parameter being tested for is larger. The Shewart p-chart is found to be less efficient than the CUSUM even when the change in parameter being tested for is large.},
	number = {3},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Grigg, O. and Farewell, V.},
	year = {2004},
	keywords = {Average run length, Cumulative sum, Performance, Resetting sequential probability ratio test, Risk-adjusted control charts, Sets method, Shewhart},
	pages = {523--539},
}

@article{griggUseRiskadjustedCUSUM2003,
	title = {Use of risk-adjusted {CUSUM} and {RSPRT} charts for monitoring in medical contexts},
	volume = {12},
	issn = {0962-2802 (Print) 0962-2802 (Linking)},
	doi = {10/gd87wh},
	abstract = {In this paper we discuss the use of charts derived from the sequential probability ratio test (SPRT): the cumulative sum (CUSUM) chart, RSPRT (resetting SPRT), and FIR (fast initial response) CUSUM. The theoretical development of the methods is described and some considerations one might address when designing a chart, explored, including the approximation of average run lengths (ARLs), the importance of detecting improvements in a process as well as detecting deterioration and estimation of the process parameter following a signal. Two examples are used to demonstrate the practical issues of quality control in the medical setting, the first a running example and the second a fully worked example at the end of the paper. The first example relates to 30-day mortality for patients of a single cardiac surgeon over the period 1994-1998, the second to patient deaths in the practice of a single GP, Harold Shipman. The charts' performances relative to each other are shown to be sensitive to the definition of the 'out of control' state of the process being monitored. In light of this, it is stressed that a suitable means by which to compare charts is chosen in any specific application.},
	number = {2},
	journal = {Statistical Methods in Medical Research},
	author = {Grigg, O. A. and Farewell, V. T. and Spiegelhalter, D. J.},
	month = mar,
	year = {2003},
	keywords = {*Medical Records, Adult, Aged, Female, Humans, Male, Middle Aged, Quality Control, Risk Adjustment/*methods, United Kingdom},
	pages = {147--70},
}

@article{lovegroveMonitoringPerformanceCardiac1999,
	title = {Monitoring the {Performance} of {Cardiac} {Surgeons}},
	volume = {50},
	issn = {01605682, 14769360},
	doi = {10/dtsvbg},
	abstract = {[In the past, simple methods have been used to examine the performance of surgeons by examining the cumulative in-hospital mortality of their patients, without regard to the severity of the cases involved. This paper describes an alternative approach which takes account of an individual cardiac surgeon's case-mix by explicitly incorporating the inherent risk faced by patients due to a combination of factors relating to their age and the degree of disease they have.]},
	number = {7},
	journal = {The Journal of the Operational Research Society},
	author = {Lovegrove, J. and Sherlaw-Johnson, C. and Valencia, O. and Treasure, T. and Gallivan, S.},
	year = {1999},
	note = {Publisher: Palgrave Macmillan Journals},
	pages = {684--689},
}

@article{mohammedPlottingBasicControl2008,
	title = {Plotting basic control charts: {Tutorial} notes for healthcare practitioners},
	volume = {17},
	doi = {10/dkz8wm},
	abstract = {There is considerable interest in the use of statistical process control (SPC) in healthcare. Although SPC is part of an overall philosophy of continual improvement, the implementation of SPC usually requires the production of control charts. However, as SPC is relatively new to healthcare practitioners and is not routinely featured in medical statistics texts/courses, there is a need to explain the issues involved in the selection and construction of control charts in practice. Following a brief overview of SPC in healthcare and preliminary issues, we use a tutorial-based approach to illustrate the selection and construction of four commonly used control charts (xmr-chart, p-chart, u-chart, c-chart) using examples from healthcare. For each control chart, the raw data, the relevant formulae and their use and interpretation of the final SPC chart are provided together with a notes section highlighting important issues for the SPC practitioner. Some more advanced topics are also mentioned with suggestions for further reading.},
	number = {2},
	journal = {Quality and Safety in Health Care},
	author = {Mohammed, M. A. and Worthington, P. and Woodall, W. H.},
	year = {2008},
	pages = {137--145},
}

@article{sherlaw-johnsonMethodDetectingRuns2005,
	title = {A {Method} for {Detecting} {Runs} of {Good} and {Bad} {Clinical} {Outcomes} on {Variable} {Life}-{Adjusted} {Display} ({VLAD}) {Charts}},
	volume = {8},
	issn = {1572-9389},
	doi = {10/dpvrft},
	abstract = {In recent years there has been a growing need for effective monitoring of clinical outcomes. Two techniques for continuous monitoring that have emerged almost simultaneously are the Variable Life-Adjusted Display (VLAD) and risk-adjusted cumulative sum charts (CUSUM). The VLAD provides clinicians and management with an easily understandable overview of outcome history and is now in routine use in several hospitals. Although it can indicate runs of good and bad outcomes, unlike the CUSUM, it does not provide a quantitative means for assessing whether they merit investigation. This paper introduces a scheme for applying control limits from CUSUM charts onto the VLAD, thus enhancing its role as an effective monitoring tool.},
	number = {1},
	journal = {Health Care Management Science},
	author = {Sherlaw-Johnson, Chris},
	month = feb,
	year = {2005},
	pages = {61--65},
}

@article{spiegelhalterFunnelPlotsComparing2005,
	title = {Funnel plots for comparing institutional performance},
	volume = {24},
	issn = {0277-6715 (Print) 0277-6715 (Linking)},
	doi = {10/fq7z8t},
	abstract = {'Funnel plots' are recommended as a graphical aid for institutional comparisons, in which an estimate of an underlying quantity is plotted against an interpretable measure of its precision. 'Control limits' form a funnel around the target outcome, in a close analogy to standard Shewhart control charts. Examples are given for comparing proportions and changes in rates, assessing association between outcome and volume of cases, and dealing with over-dispersion due to unmeasured risk factors. We conclude that funnel plots are flexible, attractively simple, and avoid spurious ranking of institutions into 'league tables'.},
	language = {english},
	number = {8},
	journal = {Statistics in Medicine},
	author = {Spiegelhalter, D. J.},
	month = apr,
	year = {2005},
	keywords = {*Biometry, Adolescent, Cardiac Surgical Procedures/mortality, Coronary Artery Bypass/mortality/statistics \& numerical data, Cross-Sectional Studies, England/epidemiology, Female, Health Care, Humans, Infant, Odds Ratio, Outcome Assessment (Health Care), Pregnancy, Pregnancy in Adolescence/prevention \& control/statistics \& numerical data, Quality Assurance, Risk Factors},
	pages = {1185--202},
}

@article{spiegelhalterHandlingOverdispersionPerformance2005,
	title = {Handling over-dispersion of performance indicators},
	volume = {14},
	doi = {10/cdmxpn},
	abstract = {Objectives: A problem can arise when a performance indicator shows substantially more variability than would be expected by chance alone, since ignoring such “over-dispersion” could lead to a large number of institutions being inappropriately classified as “abnormal”. A number of options for handling this phenomenon are investigated, ranging from improved risk stratification to fitting a statistical model that robustly estimates the degree of over-dispersion. Design: Retrospective analysis of publicly available data on survival following coronary artery bypass grafts, emergency readmission rates, and teenage pregnancies. Setting: NHS trusts in England. Results: Funnel plots clearly show the influence of the method chosen for dealing with over-dispersion on the “banding” a trust receives. Both multiplicative and additive approaches are feasible and give intuitively reasonable results, but the additive random effects formulation appears to have a stronger conceptual foundation. Conclusion: A random effects model may offer a reasonable solution. This method has now been adopted by the UK Healthcare Commission in their derivation of star ratings.},
	number = {5},
	journal = {Quality and Safety in Health Care},
	author = {Spiegelhalter, D. J.},
	year = {2005},
	pages = {347--351},
}

@article{spiegelhalterStatisticalMethodsHealthcare2012,
	title = {Statistical methods for healthcare regulation: {Rating}, screening and surveillance},
	volume = {175},
	issn = {09641998},
	doi = {10/dqpqpk},
	abstract = {Summary. Current demand for accountability and efficiency of healthcare organizations, combined with the greater availability of routine data on clinical care and outcomes, has led to an increased focus on statistical methods in healthcare regulation. We consider three different regulatory functions in which statistical analysis plays a vital role: rating organizations, deciding whom to inspect and continuous surveillance for arising problems. A common approach to data standardization based on (possibly overdispersed) Z-scores is proposed, although specific tools are used for assessing performance against a target, combining indicators when screening for inspection, and continuous monitoring using risk-adjusted sequential testing procedures. We pay particular attention to the problem of simultaneously monitoring over 200000 indicators for excess mortality, both with respect to the statistical issues surrounding massive multiplicity, and the organizational aspects of dealing with such a complex but high profile process.},
	number = {1},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Spiegelhalter, David and Sherlaw-Johnson, Christopher and Bardsley, Martin and Blunt, Ian and Wood, Christopher and Grigg, Olivia},
	year = {2012},
	keywords = {Funnel plot, League table, Overdispersion, Risk-adjusted cumulative sum, Risk-based inspection, Statistical process control},
	pages = {1--47},
}

@article{steinerMonitoringSurgicalPerformance2000,
	title = {Monitoring surgical performance using risk-adjusted cumulative sum charts},
	volume = {1},
	issn = {1465-4644},
	doi = {10/bstchx},
	abstract = {The cumulative sum (CUSUM) procedure is a graphical method that is widely used for quality monitoring in industrial settings. More recently it has been used to monitor surgical outcomes whereby it `signals' if sufficient evidence has accumulated that there has been a change in the surgical failure rate. A limitation of the standard CUSUM procedure in this context is that since it is simply based on the observed surgical outcomes, it may signal as a result of changes in the referral pattern, such as an increased proportion of high-risk patients, rather than due to a change in the actual surgical performance. We describe a new CUSUM procedure that adjusts for each patient's pre-operative risk of surgical failure through the use of a likelihood-based scoring method. The procedure is therefore ideally suited for settings where there is a variable mix of patients over time.*To whom correspondence should be addressed},
	number = {4},
	journal = {Biostatistics (Oxford, England)},
	author = {Steiner, Stefan H. and Cook, Richard J. and Farewell, Vern T. and Treasure, Tom},
	month = dec,
	year = {2000},
	pages = {441--452},
}

@article{ulmSimpleMethodCalculate1990,
	title = {Simple {Method} to {Calculate} the {Confidence} {Interval} of a {Standardized} {Mortality} {Ratio} ({SMR})},
	volume = {131},
	issn = {0002-9262},
	doi = {10/gfz5xz},
	abstract = {In analyzing standardized mortality ratios (SMRs), it is of interest to calculate a confidence interval for the true SMR. The exact limits of a specific interval can be obtained by means of the Poisson distribution either within an iterative procedure or by one of the tables. The limits can be approximated in using one of various shortcut methods. In this paper, a method is described for calculating the exact limits in a simple and easy way. The method is based on the link between the x2 distribution and the Poisson distribution. Only a table of the x2 distribution is necessary.},
	number = {2},
	journal = {American Journal of Epidemiology},
	author = {Ulm, K.},
	year = {1990},
	pages = {373--375},
}

@article{woodallDesignCUSUMQuality1986,
	title = {The {Design} of {CUSUM} {Quality} {Control} {Charts}},
	volume = {18},
	issn = {0022-4065},
	doi = {10/gjzktw},
	abstract = {Recent methods of designing CUSUM quality control charts are reviewed. It is shown that the statistical performance of control procedures obtained using economic models is often unsatisfactory.},
	number = {2},
	journal = {Journal of Quality Technology},
	author = {Woodall, William H.},
	month = apr,
	year = {1986},
	pages = {99--102},
}

@article{woodallInertialPropertiesQuality2005,
	title = {The {Inertial} {Properties} of {Quality} {Control} {Charts}},
	volume = {47},
	issn = {00401706},
	doi = {10/frj5mg},
	abstract = {[Many types of control charts have an ability to detect process changes that can weaken over time depending on the past data observed. This is often referred to as the "inertia problem." We propose a new measure of inertia, the signal resistance, to be the largest standardized deviation from target not leading to an immediate out-of-control signal. We calculate the signal resistance values for several types of univariate and multivariate charts. Our conclusions support the recommendation that Shewhart limits should be used with exponentially weighted moving average charts, especially when the smoothing parameter is small.]},
	number = {4},
	journal = {Technometrics : a journal of statistics for the physical, chemical, and engineering sciences},
	author = {Woodall, William H. and Mahmoud, Mahmoud A.},
	year = {2005},
	pages = {425--436},
}

@article{woodallUseControlCharts2006,
	title = {The {Use} of {Control} {Charts} in {Health}-{Care} and {Public}-{Health} {Surveillance}},
	volume = {38},
	issn = {0022-4065},
	doi = {10/gjzktt},
	abstract = {There are many applications of control charts in health-care monitoring and in public-health surveillance. We introduce these applications to industrial practitioners and discuss some of the ideas that arise that may be applicable in industrial monitoring. The advantages and disadvantages of the charting methods proposed in the health-care and public-health areas are considered. Some additional contributions in the industrial statistical process control literature relevant to this area are given. There are many application and research opportunities available in the use of control charts for health-related monitoring.},
	number = {2},
	journal = {Journal of Quality Technology},
	author = {Woodall, William H.},
	month = apr,
	year = {2006},
	pages = {89--104},
}

@article{aderySimplifiedMonteCarlo1968,
	title = {A {Simplified} {Monte} {Carlo} {Significance} {Test} {Procedure}},
	volume = {30},
	issn = {00359246},
	doi = {10/gfvvc3},
	abstract = {[The use of Monte Carlo test procedures for significance testing, with smaller reference sets than are now generally used, is advocated. It is shown that, for given α = 1/n, n a positive integer, the power of the Monte Carlo test procedure is a monotone increasing function of the size of the reference set, the limit of which is the power of the corresponding uniformly most powerful test. The power functions and efficiency of the Monte Carlo test to the uniformly most powerful test are discussed in detail for the case where the test criterion is N(γ. 1). The cases when the test criterion is Student's t-statistic and when the test statistic is exponentially distributed are considered also.]},
	number = {3},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Adery, C. A. Hope},
	year = {1968},
	pages = {582--598},
}

@unpublished{bolkerDealingQuasiModels2017,
	address = {CRAN},
	type = {Tutorial},
	title = {Dealing with quasi- models in {R}},
	author = {Bolker, Benjamin M.},
	month = oct,
	year = {2017},
}

@incollection{cameronGeneralizedCountRegression2013,
	address = {Cambridge},
	edition = {2},
	series = {Econometric {Society} {Monographs}},
	title = {Generalized {Count} {Regression}},
	isbn = {978-1-107-01416-9},
	abstract = {INTRODUCTIONThe most commonly used models for count regression, Poisson and negative binomial, were presented in Chapter 3. In this chapter we introduce richer models for count regression using cross-section data. For some of these models the conditional mean retains the exponential functional form. Then the Poisson QMLE and NB2 ML estimators remain consistent, although they may be inefficient and may not be suitable for predicting probabilities, rather than the conditional mean. For many of these models, however, the Poisson and NB2 estimators are inconsistent. Then alternative methods are used, ones that generally rely heavily on parametric assumptions.One reason for the failure of the Poisson regression is that the Poisson process has unobserved heterogeneity that contributes additional randomness. This leads to mixture models, the negative binomial being only one example. A second reason is the failure of the Poisson process assumption and its replacement by a more general stochastic process.Some common departures from the standard Poisson regression are as follows.Failure of the mean-equals-variance restriction: Frequently the conditional variance of data exceeds the conditional mean, which is usually referred to as extra-Poisson variation or overdispersion relative to the Poisson model. Overdispersion may result from neglected or unobserved heterogeneity that is inadequately captured by the covariates in the conditional mean function. It is common to allow for random variation in the Poisson conditional mean by introducing a multiplicative error term. This leads to families of mixed Poisson models.[…]},
	booktitle = {Regression {Analysis} of {Count} {Data}},
	publisher = {Cambridge University Press},
	author = {Cameron, A. C. and Trivedi, P. K.},
	editor = {Cameron, Colin A. and Trivedi, Pravin K.},
	year = {2013},
	pages = {111--176},
}

@article{efronBootstrapMethodsAnother1979,
	title = {Bootstrap {Methods}: {Another} {Look} at the {Jackknife}},
	volume = {7},
	issn = {0090-5364},
	doi = {10/dj84pt},
	abstract = {We discuss the following problem: given a random sample \${\textbackslash}mathbf\{X\} = (X\_1, X\_2, {\textbackslash}cdots, X\_n)\$ from an unknown probability distribution \$F\$, estimate the sampling distribution of some prespecified random variable \$R({\textbackslash}mathbf\{X\}, F)\$, on the basis of the observed data \${\textbackslash}mathbf\{x\}\$. (Standard jackknife theory gives an approximate mean and variance in the case \$R({\textbackslash}mathbf\{X\}, F) = {\textbackslash}theta({\textbackslash}hat\{F\}) - {\textbackslash}theta(F), {\textbackslash}theta\$ some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
	language = {english},
	number = {1},
	journal = {The Annals of Statistics},
	author = {Efron, B.},
	month = jan,
	year = {1979},
	keywords = {bootstrap, discriminant analysis, error rate estimation, Jackknife, nonlinear regression, nonparametric variance estimation, resampling, subsample values},
	pages = {1--26},
}

@article{famoyeRestrictedGeneralizedPoisson1993,
	title = {Restricted generalized poisson regression model},
	volume = {22},
	issn = {0361-0926},
	doi = {10/dkr9nc},
	abstract = {The family of generalized Poisson distribution has been found useful in describing over-dispersed and under-dispersed count data. We propose the use of restricted generalized Poisson regression model to predict a response variable affected by one or more explanatory variables. Approximate tests for the adequacy of the model and the estimation of the parameters are considered. Restricted generalized Poisson regression model has been applied to an observed data set.},
	number = {5},
	journal = {Communications in Statistics - Theory and Methods},
	author = {Famoye, Felix},
	month = jan,
	year = {1993},
	pages = {1335--1354},
}

@article{nelderGeneralizedLinearModels1972,
	title = {Generalized {Linear} {Models}},
	volume = {135},
	issn = {00359238},
	doi = {10/dhq253},
	abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
	number = {3},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	author = {Nelder, J. A. and Wedderburn, R. W. M.},
	year = {1972},
	pages = {370--384},
}

@article{poortemaModellingOverdispersionCounts1999,
	title = {On modelling overdispersion of counts},
	volume = {53},
	issn = {0039-0402},
	doi = {10/fxf4p2},
	abstract = {For counts it often occurs that the observed variance exceeds the nominal variance of the claimed binomial, multinomial or Poisson distributions. We study how models can be extended to cope with this phenomenon: a survey of literature is given. We focus on modelling, not on estimation or testing statistical hypotheses. The attention is restricted to independent observations.},
	number = {1},
	journal = {Statistica Neerlandica},
	author = {Poortema, K.},
	month = mar,
	year = {1999},
	keywords = {extra-binomial variation, extra-multinomial variation, extra-Poisson variation, models},
	pages = {5--20},
}

@article{sellersFlexibleRegressionModel2010,
	title = {A flexible regression model for count data},
	volume = {4},
	issn = {1932-6157},
	doi = {10/cn6mjv},
	abstract = {Poisson regression is a popular tool for modeling count data and is applied in a vast array of applications from the social to the physical sciences and beyond. Real data, however, are often over- or under-dispersed and, thus, not conducive to Poisson regression. We propose a regression model based on the Conway-Maxwell-Poisson (COM-Poisson) distribution to address this problem. The COM-Poisson regression generalizes the well-known Poisson and logistic regression models, and is suitable for fitting count data with a wide range of dispersion levels. With a GLM approach that takes advantage of exponential family properties, we discuss model estimation, inference, diagnostics, and interpretation, and present a test for determining the need for a COM-Poisson regression over a standard Poisson regression. We compare the COM-Poisson to several alternatives and illustrate its advantages and usefulness using three data sets with varying dispersion.},
	language = {english},
	number = {2},
	journal = {Annals of Applied Statistics},
	author = {Sellers, Kimberly F. and Shmueli, Galit},
	month = jun,
	year = {2010},
	keywords = {Conway-Maxwell-Poisson (COM-Poisson) distribution, dispersion, generalized linear models (GLM), generalized Poisson},
	pages = {943--961},
}


@article{shmueliExplainPredict2010,
	title = {To {Explain} or to {Predict}?},
	volume = {25},
	issn = {0883-4237},
	doi = {10/dvwwrp},
	abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
	language = {english},
	number = {3},
	journal = {Statistical Science},
	author = {Shmueli, Galit},
	month = aug,
	year = {2010},
	keywords = {causality, data mining, Explanatory modeling, predictive modeling, predictive power, scientific research, statistical strategy},
	pages = {289--310},
}

@article{skondralLatentVariableModelling2007,
	title = {Latent {Variable} {Modelling}: {A} {Survey}*},
	volume = {34},
	doi = {10/bjdkgh},
	abstract = {Abstract. Latent variable modelling has gradually become an integral part of mainstream statistics and is currently used for a multitude of applications in different subject areas. Examples of `traditional' latent variable models include latent class models, item–response models, common factor models, structural equation models, mixed or random effects models and covariate measurement error models. Although latent variables have widely different interpretations in different settings, the models have a very similar mathematical structure. This has been the impetus for the formulation of general modelling frameworks which accommodate a wide range of models. Recent developments include multilevel structural equation models with both continuous and discrete latent variables, multiprocess models and nonlinear latent variable models.},
	number = {4},
	journal = {Scandinavian Journal of Statistics},
	author = {Skondral, Anders and Rabe-Hesketh, Sophia},
	year = {2007},
	pages = {712--745},
}

@article{spiegelhalterFunnelPlotsComparing2005,
	title = {Funnel plots for comparing institutional performance},
	volume = {24},
	issn = {0277-6715 (Print) 0277-6715 (Linking)},
	doi = {10/fq7z8t},
	abstract = {'Funnel plots' are recommended as a graphical aid for institutional comparisons, in which an estimate of an underlying quantity is plotted against an interpretable measure of its precision. 'Control limits' form a funnel around the target outcome, in a close analogy to standard Shewhart control charts. Examples are given for comparing proportions and changes in rates, assessing association between outcome and volume of cases, and dealing with over-dispersion due to unmeasured risk factors. We conclude that funnel plots are flexible, attractively simple, and avoid spurious ranking of institutions into 'league tables'.},
	language = {english},
	number = {8},
	journal = {Statistics in Medicine},
	author = {Spiegelhalter, D. J.},
	month = apr,
	year = {2005},
	keywords = {*Biometry, Adolescent, Cardiac Surgical Procedures/mortality, Coronary Artery Bypass/mortality/statistics \& numerical data, Cross-Sectional Studies, England/epidemiology, Female, Health Care, Humans, Infant, Odds Ratio, Outcome Assessment (Health Care), Pregnancy, Pregnancy in Adolescence/prevention \& control/statistics \& numerical data, Quality Assurance, Risk Factors},
	pages = {1185--202},
}

@article{spiegelhalterHandlingOverdispersionPerformance2005,
	title = {Handling over-dispersion of performance indicators},
	volume = {14},
	doi = {10/cdmxpn},
	abstract = {Objectives: A problem can arise when a performance indicator shows substantially more variability than would be expected by chance alone, since ignoring such “over-dispersion” could lead to a large number of institutions being inappropriately classified as “abnormal”. A number of options for handling this phenomenon are investigated, ranging from improved risk stratification to fitting a statistical model that robustly estimates the degree of over-dispersion. Design: Retrospective analysis of publicly available data on survival following coronary artery bypass grafts, emergency readmission rates, and teenage pregnancies. Setting: NHS trusts in England. Results: Funnel plots clearly show the influence of the method chosen for dealing with over-dispersion on the “banding” a trust receives. Both multiplicative and additive approaches are feasible and give intuitively reasonable results, but the additive random effects formulation appears to have a stronger conceptual foundation. Conclusion: A random effects model may offer a reasonable solution. This method has now been adopted by the UK Healthcare Commission in their derivation of star ratings.},
	number = {5},
	journal = {Quality and Safety in Health Care},
	author = {Spiegelhalter, D. J.},
	year = {2005},
	pages = {347--351},
}

@book{venablesModernAppliedStatistics2013,
	title = {Modern {Applied} {Statistics} with {S}-{Plus}},
	isbn = {978-1-4899-2820-7},
	publisher = {Springer New York},
	author = {Venables, W. N. and Ripley, B. D.},
	year = {2013},
}

@article{wedderburnQuasiLikelihoodFunctionsGeneralized1974,
	title = {Quasi-{Likelihood} {Functions}, {Generalized} {Linear} {Models}, and the {Gauss}-{Newton} {Method}},
	volume = {61},
	issn = {00063444},
	doi = {10/dvvx57},
	abstract = {To define a likelihood we have to specify the form of distribution of the observations, but to define a quasi-likelihood function we need only specify a relation between the mean and variance of the observations and the quasi-likelihood can then be used for estimation. For a one-parameter exponential family the log likelihood is the same as the quasi-likelihood and it follows that assuming a one-parameter exponential family is the weakest sort of distributional assumption that can be made. The Gauss-Newton method for calculating nonlinear least squares estimates generalizes easily to deal with maximum quasi-likelihood estimates, and a rearrangement of this produces a generalization of the method described by Nelder \& Wedderburn (1972).},
	number = {3},
	journal = {Biometrika},
	author = {Wedderburn, R. W. M.},
	year = {1974},
	pages = {439--447},
}

@phdthesis{maineyStatisticalMethodsNHS2020,
	address = {London},
	title = {Statistical methods for {NHS} incident reporting data},
	abstract = {The National Reporting and Learning System (NRLS) is the English and Welsh NHS' national repository of incident reports from healthcare. It aims to capture details of incident reports, at national level, and facilitate clinical review and learning to improve patient safety. These incident reports range from minor `near-misses' to critical incidents that may lead to severe harm or death. NRLS data are currently reported as crude counts and proportions, but their major use is clinical review of the free-text descriptions of incidents. There are few well-developed quantitative analysis approaches for NRLS, and this thesis investigates these methods. A literature review revealed a wealth of clinical detail, but also systematic constraints of NRLS' structure, including non-mandatory reporting, missing data and misclassification. Summary statistics for reports from 2010/11 – 2016/17 supported this and suggest NRLS was not suitable for statistical modelling in isolation. Modelling methods were advanced by creating a hybrid dataset using other sources of hospital casemix data from Hospital Episode Statistics (HES). A theoretical model was established, based on `exposure' variables (using casemix proxies), and `culture' as a random-effect. The initial modelling approach examined Poisson regression, mixture and multilevel models. Overdispersion was significant, generated mainly by clustering and aggregation in the hybrid dataset, but models were chosen to reflect these structures. Further modelling approaches were examined, using Generalized Additive Models to smooth predictor variables, regression tree-based models including Random Forests, and Artificial Neural Networks. Models were also extended to examine a subset of death and severe harm incidents, exploring how sparse counts affect models. Text mining techniques were examined for analysis of incident descriptions and showed how term frequency might be used. Terms were used to generate latent topics models used, in-turn, to predict the harm level of incidents. Model outputs were used to create a `Standardised Incident Reporting Ratio' (SIRR) and cast this in the mould of current regulatory frameworks, using process control techniques such as funnel plots and cusum charts. A prototype online reporting tool was developed to allow NHS organisations to examine their SIRRs, provide supporting analyses, and link data points back to individual incident reports.},
	language = {english},
	school = {UCL},
	author = {Mainey, Christopher},
	year = {2020},
	URL = {https://discovery.ucl.ac.uk/id/eprint/10094736/}
}

@article{chouldechovaGeneralizedAdditiveModel2015,
	title = {Generalized {Additive} {Model} {Selection}},
	volume = {1506.03850},
	journal = {arXiv.org},
	author = {Chouldechova, Alexandra and Hastie, Trevor},
	month = jun,
	year = {2015},
	keywords = {⛔ No DOI found},
	pages = {23},
}

@article{hastieGeneralizedAdditiveModels1986,
	title = {Generalized {Additive} {Models}},
	volume = {1},
	issn = {08834237},
	abstract = {Likelihood-based regression models such as the normal linear regression model and the linear logistic model, assume a linear (or some other parametric) form for the covariates X₁, X₂, \&\#x22ef;, Xₚ. We introduce the class of generalized additive models which replaces the linear form \&\#x2211; \&\#x3b2;$_{\textrm{jXj}}$ by a sum of smooth functions \&\#x2211; s$_{\textrm{j}}$(X$_{\textrm{j}}$). The s$_{\textrm{j}}$(\&\#xb7;)'s are unspecified functions that are estimated using a scatterplot smoother, in an iterative procedure we call the local scoring algorithm. The technique is applicable to any likelihood-based regression model: the class of generalized linear models contains many of these. In this class the linear predictor \&\#x3b7; = \&\#x3a3; \&\#x3b2;$_{\textrm{jXj}}$ is replaced by the additive predictor \&\#x3a3; s$_{\textrm{j}}$(X$_{\textrm{j}}$); hence, the name generalized additive models. We illustrate the technique with binary response and survival data. In both cases, the method proves to be useful in uncovering nonlinear covariate effects. It has the advantage of being completely automatic, i.e., no "detective work" is needed on the part of the statistician. As a theoretical underpinning, the technique is viewed as an empirical method of maximizing the expected log likelihood, or equivalently, of minimizing the Kullback-Leibler distance to the true model.},
	number = {3},
	journal = {Statistical Science},
	author = {Hastie, Trevor and Tibshirani, Robert},
	year = {1986},
	keywords = {❓ Multiple DOI},
	pages = {297--310},
}

@article{reinschSmoothingSplineFunctions1967,
	title = {Smoothing by spline functions},
	volume = {10},
	issn = {0029-599X},
	doi = {10/d2ftmg},
	number = {3},
	journal = {Numerische mathematik},
	author = {Reinsch, Christian H.},
	year = {1967},
	pages = {177--183},
}

@book{woodGeneralizedAdditiveModels2006,
	title = {Generalized {Additive} {Models}: {An} {Introduction} with {R}},
	isbn = {978-1-58488-474-3},
	publisher = {Taylor \& Francis},
	author = {Wood, Simon N.},
	year = {2006},
}

@article{breimanBaggingPredictors1996,
	title = {Bagging predictors},
	volume = {24},
	issn = {1573-0565},
	doi = {10/fm3fwj},
	abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
	number = {2},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = aug,
	year = {1996},
	pages = {123--140},
}

@article{breimanRandomForests2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {08856125},
	doi = {10/d8zjwq},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	number = {1},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	year = {2001},
	pages = {5--32},
}

@article{elithWorkingGuideBoosted2008,
	title = {A working guide to boosted regression trees},
	volume = {77},
	issn = {0021-8790},
	doi = {10/fn6m6v},
	abstract = {1. Ecologists use statistical models for both explanation and prediction, and need techniques that are flexible enough to express typical features of their data, such as nonlinearities and interactions. 2. This study provides a working guide to boosted regression trees (BRT), an ensemble method for fitting statistical models that differs fundamentally from conventional techniques that aim to fit a single parsimonious model. Boosted regression trees combine the strengths of two algorithms: regression trees (models that relate a response to their predictors by recursive binary splits) and boosting (an adaptive method for combining many simple models to give improved predictive performance). The final BRT model can be understood as an additive regression model in which individual terms are simple trees, fitted in a forward, stagewise fashion. 3. Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data. They have no need for prior data transformation or elimination of outliers, can fit complex nonlinear relationships, and automatically handle interaction effects between predictors. Fitting multiple trees in BRT overcomes the biggest drawback of single tree models: their relatively poor predictive performance. Although BRT models are complex, they can be summarized in ways that give powerful ecological insight, and their predictive performance is superior to most traditional modelling methods. 4. The unique features of BRT raise a number of practical issues in model fitting. We demonstrate the practicalities and advantages of using BRT through a distributional analysis of the short-finned eel (Anguilla australis Richardson), a native freshwater fish of New Zealand. We use a data set of over 13 000 sites to illustrate effects of several settings, and then fit and interpret a model using a subset of the data. We provide code and a tutorial to enable the wider use of BRT by ecologists.},
	language = {english},
	number = {4},
	journal = {Journal of Animal Ecology},
	author = {Elith, J. and Leathwick, J. R. and Hastie, T.},
	month = jul,
	year = {2008},
	keywords = {*Models, *Statistics as Topic, Anguilla/growth \& development/*physiology, Animals, Biological, Decision Trees, Ecology/*statistics \& numerical data, Models, Regression Analysis, Statistical},
	pages = {802--13},
}

@article{friedmanStochasticGradientBoosting2002,
	title = {Stochastic {Gradient} {Boosting}},
	volume = {38},
	doi = {10/fxb956},
	abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current “pseudo”-residuals by least squares at each iteration. The pseudo-residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Specifically, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to fit the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner.},
	journal = {Computational Statistics \& Data Analysis},
	author = {Friedman, Jerome H.},
	year = {2002},
	pages = {367--378},
}

@book{goodfellowDeepLearning2018,
	address = {Cambridge, Massachutsetts \& London, England},
	series = {Adaptive {Computation} and {Machine} {Learning} series},
	title = {Deep {Learning}},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	editor = {Back, Francis},
	year = {2018},
}

@book{hastieElementsStatisticalLearning2009,
	address = {New York, NETHERLANDS},
	edition = {2},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning} : {Data} {Mining}, {Inference}, and {Prediction}},
	isbn = {978-0-387-84858-7},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	keywords = {Data mining., Electronic books. – local, Supervised learning (Machine learning)},
}

@article{leapeNatureAdverseEvents1991,
	title = {The {Nature} of {Adverse} {Events} in {Hospitalized} {Patients}},
	volume = {324},
	doi = {10/fhvzbs},
	number = {6},
	journal = {New England Journal of Medicine},
	author = {Leape, Lucian L. and Brennan, Troyen A. and Laird, Nan and Lawthers, Ann G. and Localio, A. Russell and Barnes, Benjamin A. and Hebert, Liesi and Newhouse, Joseph P. and Weiler, Paul C. and Hiatt, Howard},
	year = {1991},
	pages = {377--384},
}

@article{donaldsonOrganisationMemory2002,
	title = {An organisation with a memory},
	volume = {2},
	issn = {1470-2118 (Print) 1470-2118},
	doi = {10/gfz5w4},
	abstract = {Patient safety has been an under-recognised and under-researched concept until recently. It is now high on the healthcare quality agenda in many countries of the world including the UK. The recognition that human error is inevitable in a highly complex and technical field like medicine is a first step in promoting greater awareness of the importance of systems failure in the causation of accidents. Plane crashes are not usually caused by pilot error per se but by an amalgam of technical, environmental, organisational, social and communication factors which predispose to human error or worsen its consequences. In healthcare, the systematic investigation of error in the administration of medication will often reveal similarly complex causation. Experience and research from other sectors, in particular the airline industry, show that the impact of human error can be reduced if the necessary work is put in to detect and then remove weaknesses and vulnerabilties in the system. The NHS is putting in place a comprehensive programme to learn more effectively from adverse events and near misses. This aims to reduce the burden of the estimated 850,000 adverse events which occur in hospitals each year as well as targeting high risk areas such as medication error.},
	language = {english},
	number = {5},
	journal = {Clinical Medicine (London, England)},
	author = {Donaldson, L.},
	month = sep,
	year = {2002},
	keywords = {Awareness, Drug Labeling, Drug Packaging, Great Britain, Humans, Medical Errors/*prevention \& control/statistics \& numerical data, Risk Management/*methods, Safety Management/*methods},
	pages = {452--7},
}

@incollection{kohnErrHumanBuilding2000,
	address = {Washington (DC)},
	title = {To {Err} is {Human}: {Building} a {Safer} {Health} {System}},
	abstract = {Experts estimate that as many as 98,000 people die in any given year from medical errors that occur in hospitals. That's more than die from motor vehicle accidents, breast cancer, or AIDS–three causes that receive far more public attention. Indeed, more people die annually from medication errors than from workplace injuries. Add the financial cost to the human tragedy, and medical error easily rises to the top ranks of urgent, widespread public problems. To Err Is Human breaks the silence that has surrounded medical errors and their consequence–but not by pointing fingers at caring health care professionals who make honest mistakes. After all, to err is human. Instead, this book sets forth a national agenda–with state and local implications–for reducing medical errors and improving patient safety through the design of a safer health system. This volume reveals the often startling statistics of medical error and the disparity between the incidence of error and public perception of it, given many patients' expectations that the medical profession always performs perfectly. A careful examination is made of how the surrounding forces of legislation, regulation, and market activity influence the quality of care provided by health care organizations and then looks at their handling of medical mistakes. Using a detailed case study, the book reviews the current understanding of why these mistakes happen. A key theme is that legitimate liability concerns discourage reporting of errors–which begs the question, "How can we learn from our mistakes?" Balancing regulatory versus market-based initiatives and public versus private efforts, the Institute of Medicine presents wide-ranging recommendations for improving patient safety, in the areas of leadership, improved data collection and analysis, and development of effective systems at the level of direct patient care. To Err Is Human asserts that the problem is not bad people in health care–it is that good people are working in bad systems that need to be made safer. Comprehensive and straightforward, this book offers a clear prescription for raising the level of patient safety in American health care. It also explains how patients themselves can influence the quality of care that they receive once they check into the hospital. This book will be vitally important to federal, state, and local health policy makers and regulators, health professional licensing officials, hospital administrators, medical educators and students, health caregivers, health journalists, patient advocates–as well as patients themselves. First in a series of publications from the Quality of Health Care in America, a project initiated by the Institute of Medicine},
	language = {english},
	booktitle = {To {Err} is {Human}: {Building} a {Safer} {Health} {System}},
	author = {Kohn, L. T. and Corrigan, J. M. and Donaldson, M. S.},
	editor = {Kohn, L. T. and Corrigan, J. M. and Donaldson, M. S.},
	year = {2000},
}

@book{poissonRecherchesProbabiliteJugements1837,
	title = {Recherches sur la probabilit\'{e} des jugements en mati\'{e}re criminelle et en mati\`{e}re civile: pr\'{e}c\'{e}d\'{e}es des r\`{e}gles g\'{e}n\'{e}rales du calcul des probabilit\'{e}s},
	publisher = {Bachelier},
	author = {Poisson, S. D.},
	year = {1837},
}


@article{spiegelhalterHandlingOverdispersionPerformance2005,
	title = {Handling over-dispersion of performance indicators},
	volume = {14},
	doi = {10/cdmxpn},
	abstract = {Objectives: A problem can arise when a performance indicator shows substantially more variability than would be expected by chance alone, since ignoring such “over-dispersion” could lead to a large number of institutions being inappropriately classified as “abnormal”. A number of options for handling this phenomenon are investigated, ranging from improved risk stratification to fitting a statistical model that robustly estimates the degree of over-dispersion. Design: Retrospective analysis of publicly available data on survival following coronary artery bypass grafts, emergency readmission rates, and teenage pregnancies. Setting: NHS trusts in England. Results: Funnel plots clearly show the influence of the method chosen for dealing with over-dispersion on the “banding” a trust receives. Both multiplicative and additive approaches are feasible and give intuitively reasonable results, but the additive random effects formulation appears to have a stronger conceptual foundation. Conclusion: A random effects model may offer a reasonable solution. This method has now been adopted by the UK Healthcare Commission in their derivation of star ratings.},
	number = {5},
	journal = {Quality and Safety in Health Care},
	author = {Spiegelhalter, D. J.},
	year = {2005},
	pages = {347--351},
}

@article{bleiLatentDirichletAllocation2003,
	title = {Latent dirichlet allocation},
	volume = {3},
	issn = {1532-4435},
	journal = {J. Mach. Learn. Res.},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	year = {2003},
	keywords = {⛔ No DOI found},
	pages = {993--1022},
}

@article{bottlePredictingFalseAlarm2011,
	title = {Predicting the false alarm rate in multi-institution mortality monitoring},
	volume = {62},
	issn = {01605682, 14769360},
	doi = {10/cbr4rq},
	abstract = {Statistical process control is increasingly used by single hospitals or centres to monitor their performance, but national monitoring across multiple centres, measures and groups incurs higher false alarm rates unless the method is modified. We consider setting the threshold for cumulative sum charts to produce the desired false alarm rate, taking into account the centre volume and expected outcome rate. We used simulation to estimate the false alarm and successful detection rates for a variety of chart thresholds. We thereby calculated the 'cost' of a higher threshold compared with one set to give a false alarm rate of 5\% for three clinical groups of common interest. The false alarm rate often showed non-linear relations with the threshold, volume and expected mortality rate but an equation was found with good approximation to the simulated values. The relation between these factors and the 'cost' of a higher threshold was not straightforward. The 'cost' (difference in number of deaths) incurred by raising the chart threshold provides an intuitive measure and is applicable to other settings.},
	number = {9},
	journal = {The Journal of the Operational Research Society},
	author = {Bottle, A. and Aylin, P.},
	year = {2011},
	pages = {1711--1718},
}

@article{jarmanExplainingDifferencesEnglish1999,
	title = {Explaining differences in {English} hospital death rates using routinely collected data},
	volume = {318},
	issn = {0959-8138 1468-5833},
	doi = {10/fkkfm9},
	abstract = {Objectives: To ascertain hospital inpatient mortality in England and to determine which factors best explain variation in standardised hospital death ratios. Design: Weighted linear regression analysis of routinely collected data over four years, with hospital standardised mortality ratios as the dependent variable. Setting: England. Subjects:Eight million discharges from NHS hospitals when the primary diagnosis was one of the diagnoses accounting for 80\% of inpatient deaths. Main outcome measures: Hospital standardised mortality ratios and predictors of variations in these ratios. Results: The four year crude death rates varied across hospitals from 3.4\% to 13.6\% (average for England 8.5\%), and standardised hospital mortality ratios ranged from 53 to 137 (average for England 100). The percentage of cases that were emergency admissions (60\% of total hospital admissions) was the best predictor of this variation in mortality, with the ratio of hospital doctors to beds and general practitioners to head of population the next best predictors. When analyses were restricted to emergency admissions (which covered 93\% of all patient deaths analysed) number of doctors per bed was the best predictor. Conclusion: Analysis of hospital episode statistics reveals wide variation in standardised hospital mortality ratios in England. The percentage of total admissions classified as emergencies is the most powerful predictor of variation in mortality. The ratios of doctors to head of population served, both in hospital and in general practice, seem to be critical determinants of standardised hospital death rates; the higher these ratios, the lower the death rates in both cases.\%U http://www.bmj.com/content/bmj/318/7197/1515.full.pdf},
	number = {7197},
	journal = {BMJ},
	author = {Jarman, B. and Gault, S. and Alves, B. and Hider, A. and Dolan, S. and Cook, A. and Hurwitz, B. and Iezzoni, L. I.},
	year = {1999},
	pages = {1515--1520},
}

@article{campbellDevelopingSummaryHospital2012,
	title = {Developing a summary hospital mortality index: retrospective analysis in {English} hospitals over five years},
	volume = {344},
	doi = {10/gb3r9t},
	abstract = {Objectives To develop a transparent and reproducible measure for hospitals that can indicate when deaths in hospital or within 30 days of discharge are high relative to other hospitals, given the characteristics of the patients in that hospital, and to investigate those factors that have the greatest effect in changing the rank of a hospital, whether interactions exist between those factors, and the stability of the measure over time.Design Retrospective cross sectional study of admissions to English hospitals.Setting Hospital episode statistics for England from 1 April 2005 to 30 September 2010, with linked mortality data from the Office for National Statistics.Participants 36.5 million completed hospital admissions in 146 general and 72 specialist trusts.Main outcome measures Deaths within hospital or within 30 days of discharge from hospital.Results The predictors that were used in the final model comprised admission diagnosis, age, sex, type of admission, and comorbidity. The percentage of people admitted who died in hospital or within 30 days of discharge was 4.2\% for males and 4.5\% for females. Emergency admissions comprised 75\% of all admissions and 5.5\% died, in contrast to 0.8\% who died after an elective admission. The percentage who died with a Charlson comorbidity score of 0 was 2\% in contrast with 15\% who died with a score greater than 5. Given these variables, the relative standardised mortality rates of the hospitals were not noticeably changed by adjusting for the area level deprivation and number of previous emergency visits to hospital. There was little evidence that including interaction terms changed the relative values by any great amount. Using these predictors the summary hospital mortality index (SHMI) was derived. For 2007/8 the model had a C statistic of 0.911 and accounted for 81\% of the variability of between hospital mortality. A random effects funnel plot was used to identify outlying hospitals. The outliers from the SHMI over the period 2005-10 have previously been identified using other mortality indicators. Conclusion The SHMI is a relatively simple tool that can be used in conjunction with other information to identify hospitals that may need further investigation.},
	journal = {BMJ},
	author = {Campbell, Michael J. and Jacques, Richard M. and Fotheringham, James and Maheswaran, Ravi and Nicholl, Jon},
	year = {2012},
	pages = {e1001},
}
